---
title: "NESMR Log-Det Acyclicity"
author: "Stefan Eng"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
bibliography: NESMR.bib
---

```{r setup}
#| echo: false
knitr::opts_chunk$set(echo = F, warning = F, message = F)
```

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(kableExtra)
library(igraph)
devtools::load_all("~/Projects/esmr/")

effect_size_scale <- 0.2
h2 <- 0.3
J <- 5000
N <- 20000
pi_J <- 0.1
alpha <- 5e-8

G <- matrix(
  c(0, 0, 0, 0, 0,
    sqrt(0.3), 0, 0, 0, 0,
    0, 0, 0, 0, 0,
    0, -1*sqrt(0.1), 0, 0, 0,
    0, -1*sqrt(0.1), sqrt(0.2), sqrt(0.25), 0),
  nrow = 5,
  byrow = 5
) * effect_size_scale

G_total <- direct_to_total(G)

# Replicates
K <- 100

# Full network without the diagonal
B_lower <- lower.tri(G) + 0
B_full <- matrix(1, nrow = 5, ncol = 5)
diag(B_full) <- 0

# Compute which edges should be null in direct and total G
lower_null <- matrix_to_edgelist(G, lower_tri = TRUE) %>%
  mutate(
    null_edge = value == 0,
    lower_edge = TRUE
  ) %>%
  select(-value)

upper_null <- matrix_to_edgelist(upper.tri(G), lower_tri = FALSE) %>%
  filter(value) %>%
  mutate(
    null_edge = TRUE,
    lower_edge = FALSE
  ) %>%
  select(-value)

lower_null_tot <- matrix_to_edgelist(G_total, lower_tri = TRUE) %>%
  mutate(
    null_edge = value == 0,
    lower_edge = TRUE
  ) %>%
  select(-value)

null_edges <- bind_rows(
  lower_null,
  upper_null
)

null_edges_tot <- bind_rows(
  lower_null_tot,
  upper_null
)
```
## Introduction

### Overall goals

1. Identify highest likelihood DAG and estimate direct effects within that DAG.
2. Summarize uncertainty about DAG, potentially as posterior probabilities.

### Internal (NESMR-based) strategies

1. (gold standard) Enumerate all complete DAGs. Fit each with NESMR. Find the highest likelihood DAG to satisfy goal (1). Compute posterior probabilities as
$$P(DAG \mid \text{data}) = \frac{P(\text{data} \mid DAG) P(DAG)}{\sum_{DAGS} P(\text{data} \mid DAG_j) P(DAG_j)}$$
We could use a flat prior over the orderings (complete DAGs).
  - **Note**: This is not include in this document but is implemented and will be testing with 4 node case.
2. (previous version) Compute the p x p total effects matrix using p applications of MVMR (lets call this the MVMR-all total effects matrix). Use the minimum arc set strategy to identify the best graph. Refit NESMR to the best graph.
3. (log det strategy version 1) Compute the MVMR-all total effects matrix. Project to the closest DAG, refit this DAG with NESMR. Use bootstrap resampling to obtain a confidence set of DAGs. It would be interesting to see how the frequency of alternative DAGs compares to the posterior probabilities computed in 1.
4. (log det strategy version 2) Same as above but start with the NESMR-all total effects matrix.
5. (graph cML analog versions 1 and 2) Compute MVMR-all or NESMR-all total effects matrix. Apply total to direct transformation. Either stop here and simply report effects or project into nearest DAG using the log det strategy.

## Strategy
I think there is some preliminary work we want to see before we launch into a full comparison of these. Here are the leftovers from questions that came up previously that it would be good to address first.  

1. You previously looked at our standard 5 node example graph but I had some questions about information that wasn't presented. These were:
	1. How does the NESMR-all graph compare to the MVMR-all graph on all edges (not just non-zeros)?
	3. How does the graph that you get when you use the log det strategy to project either of these to the nearest DAG look compared to the truth.
	4. How does this look compared to the feedback arc set graph?

It would be nice to see answers to these questions for at least one iteration before launching into a large scale comparison.

2. In order to compare strategies 1-5, you need a setting where finding the correct graph is somewhat difficult or where there are multiple consistent orderings, with probably five or fewer total traits. I am thinking that three node graphs may always be too easy but this hunch could be wrong. You may want to try 4 traits. I think you can probably make the problem harder by making effects smaller. You might also try making some effects antagonistic (negative confounding with positive direct effects). I am not sure if this will make it harder or not. To be able to tell if the problem is "hard", you can just run method (1) and look at the posterior probabilities. I think we'd like an example where there is an incorrect ordering that has a posterior probability more than a few percent if we can find one.

## Background

## NESMR
NESMR can be broken up into two main components:

1. Estimating a given network
2. Finding the optimal network structure

### 1. Estimation

Problem 1 is largely solved by the empirical Bayes approach that esmr uses.

### 2. Optimal network structure

For small networks, the optimal network structure can be found by brute force.
For a set of $n$ traits $X_1, \ldots, X_n$ we need to first find the ordering such that a a lower triangular representation is possible.
We can then select the ordering that has the largest (log)likelihood and select this as the final network structure.
We are able to consistently estimate the causal effects using (1) with a full lower triangular matrix corresponding to the highest likelihood ordering.
Some orderings may have similar likelihoods which makes selecting a bit more difficult but some heuristics can be used to select close together orderings.
For larger networks, this is not feasible as the number of possible orderings grows factorially with the number of traits.

Our original idea was to use an algorithm that fit $n$ esmr (mvmr) steps to estimate the direct effects of $X_{-j}$ on $X_j$ for all $j = 1,\ldots, n$.
We then build a matrix of the direct mvmr causal direct effects.
This matrix is not necessarily a DAG and we need to find a DAG that is consistent with this matrix as well as a way to quantify the uncertainty in the network structure.
We fit a weighted feedback arc set algorithm (maximum acyclic subgraph) on this matrix to find the DAG that has the highest -log10 p-values.
It is possible that we could modify this algorithm to traverse the graph space using rejection sampling for a Bayesian approach to finding how often a given edge is present in the optimal network structure.

An alternative method of enforcing acyclicity is to use a continuous, differentiable penalty that is exactly zero when the matrix is acyclic and increases as the matrix becomes "more cyclic".
There are three recently proposed functions that can be used to enforce acyclicity in a continuous manner. First, let $W \circ W$ be the Hadamard product (elementwise product).
Let $W \in \mathbb R^{d \times d}$ be the weighted adjacency matrix

The first acyclicity function $h_{expm}$ is from [@zheng2018]:
A weighted adjacency matrix $W \in \mathbb R^{d \times d}$ is a DAG if and only if

$$
\begin{aligned}
  h_{expm}(W) &= tr\left( e^{W \circ W} \right) - d = 0\\
  \nabla h_{expm}(W) &= \left( e^{W \circ W} \right)^T \circ 2W
\end{aligned}
$$

The second acyclicity function $h_{poly}$ is from [@yu2019] which can be computed faster than the matrix exponential:

$$
\begin{aligned}
  h_{poly}(W) &= tr\left( \left(I + \frac{1}{d} W \circ W \right)^d\right) - d = 0
\end{aligned}
$$

The third acyclicity function $h_{ldet}$ is from [@bello2023].

Define $\mathbb W^s = \{ W \in \mathbb R^{d \times d} : s > \rho(W \circ W) \}$.

Let $s > 0$ and let $h_{ldet}^s : \mathbb W^s \to \mathbb R$ be defined as

$$
\begin{aligned}
h_{ldet}^s(W) = -\log \det(sI - W \circ W) + d \log s\\
\nabla h_{ldet}^s(W) = 2(sI - W \circ W)^{-T} \circ W
\end{aligned}
$$

Both $h_{ldet}^s(W) = 0$ if and only if $W$ is a DAG and $\nabla h_{ldet}^s(W) = 0$ if and only if $W$ is a DAG.
The authors note that $h_{ldet}$ has nicer properties and is more computationally efficient than $h_{expm}$ and $h_{poly}$.

Some of the properties of $h_{ldet}$ are

1. It is the only one of the three functions that has a tractable expression for the Hessian.
2. No large cycles are diminished by the penalty compared to $h_{expm}$ or $h_{poly}$ which means that it works better on larger graphs.
3. Both $h_{ldet}$ and $\nabla h_{ldet}$ can be computed faster than comparable DAG characterizations.

## Proposed optimization using NESMR

The first step is to estimate the casual effects (total or direct) using NESMR.
Rather than fitting $n$ NESMR models, I modified NESMR to allow for all bi-direction effects $n (n - 1)$ total effects to be estimated at once by allowing the $B$ template to be a full matrix without the diagonal.
There may be some identifiablility issues (?) but so far the results seem similar to the $n$ MVMR steps when $G$ needs to be estimated.
There might be a better way to do this step but the benefit is that we have a full matrix of total effects as well a (log)likelihood that we can optimize.

## Simulation

Simulate using our standard five node example.
Fit two effect matrices:
1. NESMR-all total effects matrix
2. MVMR-all direct effects matrix

Since the NESMR effect are the _total effects_ I used the $G_{tot}$ as the true values and for the MVMR discovery network I used $G_{dir}$
the edge from $4 \to 1$ and $5 \to 1$ will have a small total effect but no direct effect.
The edge $5 \to 2$ also has a small amount of difference between total/direct effect.
This is why these effects appear only in the NESMR-all effects matrix.
In initial simulations is appears that the NESMR full matrix is able to recover the total effects about the same as the $n$ step MVMR matrix.

### Direct effects
```{r}
plot(graph.adjacency(G != 0))
```

```{r}
kableExtra::kable(
  G,
  col.names = paste0('Trait', 1:5),
  digits = 3,
  caption = "True direct effects") %>%
  kableExtra::kable_styling()
```

###  Total effects graph
```{r}
plot(graph.adjacency(G_total != 0))
```

```{r}
kableExtra::kable(
  G_total,
  col.names = paste0('Trait', 1:5),
  digits = 3,
  caption = "True total effects") %>%
  kableExtra::kable_styling()
```

```{r, eval = FALSE}
if (FALSE) {
sim_res <- replicate(K, {
  # This is weird... we get completely identical dat for 2...n simulations...
  # unless we reset the seed. Where is the seed getting set??
  set.seed(NULL)
  dat <- GWASBrewer::sim_mv(
    G = G,
    N = N,
    J = J,
    h2 = h2,
    pi = pi_J,
    sporadic_pleiotropy = TRUE,
    est_s = TRUE
  )

  Ztrue <- with(dat, beta_marg/se_beta_hat)
  pval_true <- 2*pnorm(-abs(Ztrue))
  minp <- apply(pval_true, 1, min)
  ix <- which(minp < alpha)

  # Standard NESMR with the correct ordering lower triangular matrix
  res_nesmr <- with(dat, esmr(
    beta_hat_X = beta_hat,
    se_X = s_estimate,
    variant_ix = ix,
    G = diag(5),
    direct_effect_template = B_lower,
    max_iter = 300,
    logdet_penalty = FALSE))

  # New "full" matrix of parameters
  # Still keep identity G
  res_nesmr_full <- with(dat, esmr(
    beta_hat_X = beta_hat,
    se_X = s_estimate,
    variant_ix = ix,
    G = diag(5), # required for network problem
    direct_effect_template = B_full,
    max_iter = 300,
    logdet_penalty = FALSE))

  # Build MVMR full matrix of direct effects
  MVMR_models <- lapply(seq_len(nrow(G)), function(i) {
    mvmr_minp <- apply(pval_true[,-i], 1, min)
    mvmr_ix <- which(mvmr_minp < 5e-8)

    # Estimate G at each step for fair comparison
    with(dat,
         esmr(beta_hat_Y = beta_hat[,i],
              se_Y = s_estimate[,i],
              beta_hat_X = beta_hat[,-i],
              se_X = s_estimate[,-i],
              variant_ix = mvmr_ix,
              G = NULL,
              beta_joint = TRUE)
    )
  })

  # Create matrix from the effects
  mvmr_beta_df <- do.call(
    'rbind.data.frame',
    lapply(1:5, function(i) {
      x <- MVMR_models[[i]]

      res <- x$beta[c('beta_m', 'beta_s')]
      res$to <- rep(i, 4)
      res$from <- setdiff(1:5, i)
      res
    })
  )

  mvmr_beta_edgelist <- mvmr_beta_df %>%
    select(from, to, beta_m, beta_s)

  adj_mat_beta <- matrix(0, nrow = 5, ncol = 5)
  adj_mat_beta[as.matrix(mvmr_beta_edgelist[, 1:2])] <- mvmr_beta_edgelist$beta_m

  mvmr_se <- matrix(0, nrow = 5, ncol = 5)
  mvmr_se[as.matrix(mvmr_beta_edgelist[, 1:2])] <- mvmr_beta_edgelist$beta_s

  fgbar <- t(res_nesmr_full$f$fgbar)
  fg2bar <- t(res_nesmr_full$f$fg2bar)

  list(
    #dat = dat,
    #res_nesmr = res_nesmr,
    #res_nesmr_full = res_nesmr_full,
    res_nesmr_full_effects = fgbar,
    res_nesmr_full_se = fg2bar - fgbar^2,
    mvmr_beta_edgelist = mvmr_beta_edgelist,
    mvmr_beta = adj_mat_beta,
    mvmr_se = mvmr_se
    )
  }, simplify = FALSE)

# Summarize the results
merged_res <- bind_rows(
  lapply(sim_res, function(x) {
    left_join(
      matrix_to_edgelist(x$res_nesmr_full_effects, value = 'full_beta'),
      matrix_to_edgelist(x$res_nesmr_full_effects - G_total, value = 'full_bias'),
      by = c('from', 'to')
    ) %>%
      left_join(
        matrix_to_edgelist(x$mvmr_beta, value = 'mvmr_beta'),
        by = c('from', 'to')
      ) %>%
      left_join(
        matrix_to_edgelist(x$mvmr_beta - G, value = 'mvmr_bias'),
        by = c('from', 'to')
      )
}), .id = "id") %>%
  filter(from != to)

# Add null edge information to the merged results
# merged_res <- left_join(
#     merged_res,
#     null_edges,
#     by = c('from', 'to')
#   )
saveRDS(merged_res, 'nesmr_logdet.rds')
}
```

```{r, echo = FALSE}
#| message: false
merged_res <- readRDS('nesmr_logdet.rds')
long_merged_res <- merged_res %>%
  # Convert full_bias and mvmr_bias to long format
  pivot_longer(
    cols = c(full_bias, mvmr_bias),
    names_to = 'method',
    values_to = 'bias'
  ) %>%
  select(-null_edge, -lower_edge)

all_null <- bind_rows(
  cbind(null_edges, method = 'mvmr_bias'),
  cbind(null_edges_tot, method = 'full_bias')
)

long_merged_res <- left_join(
  long_merged_res,
  all_null,
  by = c('from', 'to', 'method'))
```

### Non-null edges

```{r}
#| message: false
non_null_res <- long_merged_res %>%
  filter(!null_edge)

non_null_res %>%
  group_by(
    from, to, method
  ) %>%
  summarise(
    mean = mean(bias),
    sd = sd(bias)
  ) %>%
  pivot_wider(
    names_from = method,
    values_from = c(mean, sd)
  ) %>%
  kableExtra::kable(digits = 3) %>%
  kableExtra::kable_styling()

grouped_non_null_res <- non_null_res %>%
  filter(from != to) %>%
  group_by(from, to, method) %>%
  summarize(mean_bias = mean(bias), bias_se = sd(bias))

grouped_non_null_res$from_to <- as.factor(paste0(grouped_non_null_res$from, "->", grouped_non_null_res$to))
grouped_non_null_res$y.jitter <- as.numeric(grouped_non_null_res$from_to) + ifelse(grouped_non_null_res$method == 'full_bias', -0.1, 0.1)

(ci_plot <- ggplot(grouped_non_null_res, aes(y = y.jitter, color = method)) +
  #geom_point(aes(x = bias, y = interaction(from, to), color = method)) +
  geom_point(
    aes(x = mean_bias), shape = 17) +
  geom_segment(aes(
    x = mean_bias - 1.96/sqrt(K) * bias_se,
    xend = mean_bias + 1.96/sqrt(K) * bias_se,
    yend = y.jitter
  )) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  # Make y-axis labels the same as the interaction levels
  scale_y_continuous(
    breaks = seq_along(levels(grouped_non_null_res$from_to)),
    labels = levels(grouped_non_null_res$from_to)
  ) +
  ylab('') +
  xlab('Mean bias + 95% CI') +
  ggtitle("Non-null edges for NESMR-all vs MVMR-all complete graph") +
  theme_minimal())

ggsave('ci_plot_non_null_edges.png', ci_plot, width = 6, height = 12)

ggplot(non_null_res, aes(x = bias, color = method)) +
  facet_wrap(
    ~ interaction(from, to)
  ) + 
  geom_density() +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  geom_vline(
    data = grouped_non_null_res,
    mapping = aes(xintercept = mean_bias, color = method)
  ) +
  ggtitle("Non-null edges for NESMR-all vs MVMR-all complete graph") +
  theme_minimal()
```

### Null edges

```{r}
null_res <- long_merged_res %>%
  filter(null_edge)

null_res %>%
  group_by(
    from, to, method
  ) %>%
  summarise(
    mean = mean(bias),
    sd = sd(bias)
  ) %>%
  pivot_wider(
    names_from = method,
    values_from = c(mean, sd)
  ) %>%
  kableExtra::kable(digits = 3) %>%
  kableExtra::kable_styling()

grouped_null_res <- null_res %>%
  filter(from != to) %>%
  group_by(from, to, method) %>%
  summarize(mean_bias = mean(bias), bias_se = sd(bias))

grouped_null_res$from_to <- as.factor(paste0(grouped_null_res$from, "->", grouped_null_res$to))
grouped_null_res$y.jitter <- as.numeric(grouped_null_res$from_to) + ifelse(grouped_null_res$method == 'full_bias', -0.1, 0.1)

(ci_plot <- ggplot(grouped_null_res, aes(y = y.jitter, color = method)) +
  #geom_point(aes(x = bias, y = interaction(from, to), color = method)) +
  geom_point(
    aes(x = mean_bias), shape = 17) +
  geom_segment(aes(
    x = mean_bias - 1.96/sqrt(K) * bias_se,
    xend = mean_bias + 1.96/sqrt(K) * bias_se,
    yend = y.jitter
  )) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  # Make y-axis labels the same as the interaction levels
  scale_y_continuous(
    breaks = seq_along(levels(grouped_null_res$from_to)),
    labels = levels(grouped_null_res$from_to)
  ) +
  ylab('') +
  xlab('Mean bias + 95% CI') +
  ggtitle("Null edges for NESMR-all vs MVMR-all complete graph") +
  theme_minimal())

ggsave('ci_plot_null_edges.png', ci_plot, width = 6, height = 12)

ggplot(null_res, aes(x = bias, color = method)) +
  facet_wrap(
    ~ interaction(from, to)
  ) + 
  geom_density() +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  geom_vline(
    data = grouped_null_res,
    mapping = aes(xintercept = mean_bias, color = method)
  ) +
  ggtitle("Null edges for NESMR-all vs MVMR-all complete graph") +
  theme_minimal()
```

## Total effect to DAG

Simulate a single example using the same five node graph

```{r}
#| echo: true
#| results: 'hide'

set.seed(13)
effect_size_scale <- 0.2
h2 <- 0.3
J <- 5000
N <- 20000
pi_J <- 0.1
alpha <- 5e-8

G <- matrix(
  c(0, 0, 0, 0, 0,
    sqrt(0.3), 0, 0, 0, 0,
    0, 0, 0, 0, 0,
    0, -1*sqrt(0.1), 0, 0, 0,
    0, -1*sqrt(0.1), sqrt(0.2), sqrt(0.25), 0),
  nrow = 5,
  byrow = 5
) * effect_size_scale

# Full network without the diagonal
B_full <- matrix(1, nrow = 5, ncol = 5)
diag(B_full) <- 0
dat <- GWASBrewer::sim_mv(
  G = G,
  N = N,
  J = J,
  h2 = h2,
  pi = pi_J,
  sporadic_pleiotropy = TRUE,
  est_s = TRUE
)

Ztrue <- with(dat, beta_marg/se_beta_hat)
pval_true <- 2*pnorm(-abs(Ztrue))
minp <- apply(pval_true, 1, min)
ix <- which(minp < 5e-8)

res_nesmr_full <- with(dat, esmr(
  beta_hat_X = beta_hat,
  se_X = s_estimate,
  variant_ix = ix,
  G = diag(5), # required for network problem
  direct_effect_template = B_full,
  max_iter = 300))

total_est <- t(res_nesmr_full$f$fgbar) 
diag(total_est) <- 0
total_est_se <- res_nesmr_full$f$fg2bar - res_nesmr_full$f$fgbar^2
```

At this point, we are at a similar place to where [@lin2023] uses network deconvolution to get direct effects.
Using their total to direct projection function we can get the direct effects from the total effects but it is not clear to me how to understand the projection.

```{r}
#| echo: true
#| eval: false
#| file: ~/Projects/esmr/R/graphcml_utils.R
```

```{r}
direct_deconv <- total_to_dir_projection_gcml(total_est, maxit = 1000)
# Set to zero ?
diag(direct_deconv) <- 0
kable(direct_deconv,
      digits = 3,
      caption = 'Graph-cML total to direct effect projection',
      col.names = paste0('Trait', 1:5)) %>%
  kable_styling()
```

The first approach with log-det penalty is to directly optimize the $h_{ldet}^s$ function with the total effect estimates.
An acyclic total effects adjacency matrix will be acyclic in the direct effects as well.
Selecting $s = \max_{i,j} \{ \beta_{i,j}^2 \} + 0.1$ seems to work well in practice since we will always have $s > \rho(\beta)$.
While the optimization brings the $h_{det}^s$ _close_ to zero there are still small effects that technically make this not a DAG and need to use `zapsmall` to remove these small effects.

```{r}
#| echo: true
#| eval: false
#| file: ~/Projects/esmr/R/project_to_DAG.R
```

Then we can project the total effects to the nearest DAG using the log-det penalty.
We try to zap the least number of digits possible that still is a DAG.

```{r, echo = TRUE}
logdet_total_est <- project_to_DAG(total_est, s = max(total_est^2) + 0.1)
for (i in seq(7, 1, by = -1)) {
  logdet_total_est <- zapsmall(logdet_total_est, i)
  if (is_dag_matrix(logdet_total_est)) {
    print(sprintf("logdet total effects are a DAG with %s digits of zapping", i))
    break
  }
}
if (! is_dag_matrix(logdet_total_est)) {
  stop("logdet total effect not not a DAG event with 2 digits of zapping... something went wrong")
}

kable(logdet_total_est,
      digits = 3,
      caption = sprintf('Graph-cML total to direct effect projection with %s digits zapping', i),
      col.names = paste0('Trait', 1:5)) %>%
  kable_styling()

plot(graph.adjacency(logdet_total_est != 0))
```

## Estimating standard error in the estimates

One approach to estimating the standard error in the estimates is to use resample using the standard errors from the NESMR model and combine the results into a parametric bootstrap version.
We seem to get the same DAG in most cases (all?) in both lower and higher power cases.

```{r, echo = TRUE}
n_resample <- 100

resample_results <- replicate(n_resample, {
  resample_total_effects <- total_est + sapply(total_est_se, rnorm, n = 1, mean = 0)
  # TODO: Do we need to keep this?
  s_resample <- max(resample_total_effects^2) + 0.1
  resample_logdet_total_est <- tryCatch(
    project_to_DAG(total_est, s = s_resample),
    warning = function(e) {
      project_to_DAG(total_est, s = s_resample + 0.1)
    }
  )
  resample_logdet_total_est
}, simplify = FALSE)
```

## References
